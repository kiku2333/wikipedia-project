{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/dask/dataframe/utils.py:374: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/dask/dataframe/utils.py:374: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/dask/dataframe/utils.py:374: FutureWarning: pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as ddf\n",
    "import collections\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import mwparserfromhell\n",
    "import multiprocessing\n",
    "from Levenshtein import distance\n",
    "from difflib import SequenceMatcher\n",
    "from collections import ChainMap\n",
    "from random import randint\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetdir = '../../tcm-columns-add-main/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_editors = pd.read_parquet('../../intermediate-result/TCM/editors-with-sig-contrib-at-least-10').index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "usernames = ddf.read_parquet(os.path.join(parquetdir,'contributor.username')).compute().drop(columns={'dir0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "revision_text = ddf.read_parquet(os.path.join(parquetdir,'revision.text')).compute().drop(columns={'dir0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = revision_text.join(usernames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_ids  = ddf.read_parquet(os.path.join(parquetdir,'revision.parentid')).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(parent_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns={'dir0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df = df[df['contributor.username'].isin(selected_editors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_index = list(set(list(selected_df['revision.parentid']) + list(selected_df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_index = [i for i in kept_index if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.index.isin(kept_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get former info for references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract references for selected editors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_refs(text):\n",
    "    wikicode = mwparserfromhell.parse(text)\n",
    "    ref_list = []\n",
    "    for tag in wikicode.filter_tags():\n",
    "        if tag.tag == 'ref':\n",
    "            ref_list.append(tag.strip())\n",
    "    return '\\t'.join(ref_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['refs'] = df['revision.text'].apply(get_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns={'revision.text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "revision_text_df = df[['refs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "refs_df = ddf.from_pandas(revision_text_df,chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs_df.to_parquet('../../intermediate-result/TCM/revision.refs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find diff to get added and removed for each revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_added_removed(d,col):\n",
    "    curr_id = d.name\n",
    "    prev_id = d['revision.parentid']\n",
    "    curr_content = collections.Counter(df.loc[curr_id][col].split('\\t'))\n",
    "    prev_content = collections.Counter()\n",
    "\n",
    "    if prev_id in df.index:\n",
    "        prev_content = collections.Counter(df.loc[prev_id][col].split('\\t'))\n",
    "\n",
    "    added = curr_content - prev_content\n",
    "    removed = prev_content - curr_content\n",
    "\n",
    "    return pd.Series([added,removed],index=[col+'.added',col+'.removed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['refs.added','refs.removed']] = df.apply(find_added_removed,args=('refs',),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = ddf.from_pandas(df,npartitions=4*multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_parquet('../../intermediate-result/TCM/refs-added-removed', object_encoding='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_to_list(d):\n",
    "    refs = []\n",
    "    for k,v in d.items():\n",
    "        tags = re.findall('<ref[^>]*>[^<]*', k)\n",
    "        tag_names = [''.join(re.search('<ref[^>]*>',i)[0].split('/')) + '</ref>' for i in tags if ('/>' in i)]\n",
    "        tag_refs = [re.findall('<ref[^>]*>[^<]*', i)[0] + '</ref>' for i in tags if not ('/>' in i)]\n",
    "        tag_refs += tag_names\n",
    "        refs += tag_refs\n",
    "        \n",
    "    return refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_modification(df,col):\n",
    "    added = df[col + '.added']\n",
    "    removed = df[col + '.removed']\n",
    "    if (len(added) == 0) & (len(removed) == 0):\n",
    "        return [[],[],[]]\n",
    "    elif len(added) == 0:\n",
    "        return [[],[],convert_dict_to_list(df[col + '.removed'])]\n",
    "    elif len(removed) == 0:\n",
    "        return [convert_dict_to_list(df[col + '.added']),[],[]]\n",
    "    else:\n",
    "        # a: added list\n",
    "        # r: removed list\n",
    "        a = convert_dict_to_list(added)\n",
    "        r = convert_dict_to_list(removed)\n",
    "        \n",
    "        # some refs may be same, so remove them\n",
    "        added_list = [elem for elem in a if elem not in r]\n",
    "        removed_list = [elem for elem in r if elem not in a]\n",
    "        \n",
    "        matched_indexes = []\n",
    "        similar_pairs = []\n",
    "        for i in range(len(added_list)):\n",
    "            val = added_list[i]\n",
    "            sim = list(map(lambda x: similar(x,val),removed_list))\n",
    "            if len(sim) > 0:\n",
    "                max_sim = max(sim)\n",
    "                max_index = sim.index(max(sim))\n",
    "                if (max_sim > 0.8) and (max_index not in matched_indexes):\n",
    "                    similar_pairs.append([i,max_index])\n",
    "                    matched_indexes.append(max_index)\n",
    "        \n",
    "        add_index = [item[0] for item in similar_pairs]\n",
    "        removed_index = [item[1] for item in similar_pairs]\n",
    "        add_matched = [added_list[i] for i in add_index]\n",
    "        removed_matched = [removed_list[i] for i in removed_index]\n",
    "        \n",
    "        modified_res = [{removed_list[item[1]]:added_list[item[0]]} for item in similar_pairs]\n",
    "        added_res = list(set(added_list)^set(add_matched))\n",
    "        removed_res = list(set(removed_list)^set(removed_matched))\n",
    "        return [added_res,modified_res,removed_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['refs.added','refs.modified','refs.removed']] = df.apply(lambda d: find_modification(d,'refs'),axis=1, \\\n",
    "                                                             result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated the former info\n",
    "# some refs have different format but refers to the same content, just comparing the text could not find these refs\n",
    "# update the algorithm to find more modified pairs (added and removed refs are the same one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_title = ddf.read_parquet(os.path.join(parquetdir,'page.title')).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(page_title).drop(columns={'dir0','refs'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_talk(text):\n",
    "    return re.search('Talk:', text) != None\n",
    "df['is_talk'] = df['page.title'].apply(is_talk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['is_talk']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(c1,c2):\n",
    "    n = max(len(c1),len(c2))\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    return distance(c1.lower(),c2.lower())/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tag_name(content):\n",
    "    # remove special characters\n",
    "    wikicode = mwparserfromhell.parse(content)\n",
    "    tag = wikicode.filter_tags()\n",
    "    if len(tag) == 0:\n",
    "        # some how cannot parse properly, so use regular expression\n",
    "        sub_string = re.search(r'name=\\\"(.*?)\\\"', content)\n",
    "        if sub_string is not None:\n",
    "            return sub_string.group(1)\n",
    "        else:\n",
    "            return None\n",
    "    attrs = tag[0].attributes\n",
    "    tag_name = None\n",
    "    for attr in attrs:\n",
    "        if attr.name == 'name' and (attr.value):\n",
    "            tag_name = attr.value.strip()\n",
    "            tag_name = re.sub('[^A-Za-z0-9.]+', ' ', tag_name).strip()\n",
    "    return tag_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_same_field(a,b,field):\n",
    "    parse_a = parse_content(a)\n",
    "    parse_b = parse_content(b)\n",
    "    \n",
    "    # both have id, compare the content of the field\n",
    "    if (field in parse_a) and (field in parse_b):\n",
    "        if (parse_a[field] == parse_b[field]) and (parse_a[field] is not None) and (parse_b[field] is not None):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "#     # both no id\n",
    "    if (field not in parse_a) and (field not in parse_b):\n",
    "        return False\n",
    "    \n",
    "    search_id = ''\n",
    "    if field not in parse_b:\n",
    "        search_id = parse_a[field]\n",
    "        if search_id is not None:\n",
    "            return search_id in b.lower()\n",
    "    if field not in parse_a:\n",
    "        search_id = parse_b[field]\n",
    "        if search_id is not None:\n",
    "            return search_id in a.lower()\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_content(content):\n",
    "    wikicode = mwparserfromhell.parse(content)\n",
    "    templates = wikicode.filter_templates()\n",
    "    ref_id = None\n",
    "    data = {}\n",
    "    # could be parsed into different fields\n",
    "    if len(templates) != 0:\n",
    "        # if could find some unique identifier, use it as the id\n",
    "        cite_type = templates[0].name.strip()\n",
    "        params = templates[0].params \n",
    "        if cite_type.lower() == 'cite pmid' or cite_type.lower() == 'cite pubmed':\n",
    "            ref_id = 'pmid: ' + templates[0].params[0].strip()\n",
    "        else:  \n",
    "            # if cannot find an id, parse the content, and try to find an id (use pmid, doi, ect as id)\n",
    "            for item in params:\n",
    "                key = item.name.strip().lower()\n",
    "                key = re.sub('[^A-Za-z0-9.]+', '', key)\n",
    "                value = item.value.strip().replace('\\\\n',' ').strip().lower()\n",
    "                value = re.sub('\\[','',value)\n",
    "                value = re.sub('\\]','',value)\n",
    "                \n",
    "                data[key]= value.lower()\n",
    "                \n",
    "            if ('pmid' in data) and (data['pmid']!=''):\n",
    "                ref_id = 'pmid: ' + data['pmid']\n",
    "            elif ('doi' in data) and (data['doi'] != ''):\n",
    "                ref_id = 'doi: ' + data['doi']\n",
    "            elif ('isbn' in data) and (data['isbn'] != ''):\n",
    "                ref_id = 'isbn: ' + data['isbn']\n",
    "            elif ('id' in data) and (data['id'] != ''):\n",
    "                ref_id = data['id']\n",
    "\n",
    "        data['ref_id'] = ref_id\n",
    "\n",
    "    else:\n",
    "        # content cannot be parsed\n",
    "        # try to find url\n",
    "        url = re.findall(r'https?://[^\\s<>\";\\]]+|www\\.[^\\s<>\";\\]]+', content)\n",
    "        if len(url) > 0:\n",
    "            data['url'] = url[0]\n",
    "        content = re.sub('\\[','',content)\n",
    "        content = re.sub('\\]','',content)\n",
    "        # same all content to 'unparsed' field\n",
    "        data['unparsed'] = content.lower()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sim_pairs(a,b):\n",
    "    matched_indexes = []\n",
    "    similar_pairs = {}\n",
    "    same_pairs = {}\n",
    "    \n",
    "    for i in range(len(a)):\n",
    "        val_a = a[i]\n",
    "        for j in range(len(b)):\n",
    "            if j in matched_indexes:\n",
    "                continue\n",
    "            val_b = b[j]            \n",
    "            \n",
    "            # if dis is 0, two strings are exactly the same, it should not be an added or modified or removed ref\n",
    "            # this may caused by the format of the reference\n",
    "            # then add to same_pairs instead\n",
    "            # remove the same_pairs later, but do not add it to modified list\n",
    "            if val_a == val_b:\n",
    "                matched_indexes.append(j)\n",
    "                same_pairs[val_a] = val_b\n",
    "                break\n",
    "                       \n",
    "            same_id = has_same_field(val_a,val_b,'ref_id')\n",
    "            same_title = has_same_field(val_a,val_b,'title')\n",
    "            same_author = has_same_field(val_a,val_b,'author')\n",
    "            same_url= has_same_field(val_a,val_b,'url')\n",
    "            dis = get_distance(val_a,val_b)\n",
    "            \n",
    "            \n",
    "            if dis < 0.5 or same_id or same_title or same_author or same_url or (val_b in val_a):\n",
    "                # use removed as key and added as value, to be same with modified computed before\n",
    "#                 similar_pairs[val_a] = val_b\n",
    "                similar_pairs[val_b] = val_a\n",
    "                matched_indexes.append(j)\n",
    "                # found a match, then break the for loop for j\n",
    "                break\n",
    "            # same ref name\n",
    "            if find_tag_name(val_a) == find_tag_name(val_b):\n",
    "#                     similar_pairs[val_a] = val_b\n",
    "                similar_pairs[val_b] = val_a\n",
    "                matched_indexes.append(j)\n",
    "                break\n",
    "    add_matched = [x for x in similar_pairs.values()]\n",
    "    removed_matched = [x for x in similar_pairs.keys()]\n",
    "    same_add = [x for x in same_pairs.keys()]\n",
    "    same_removed = [x for x in same_pairs.values()]\n",
    "    return [similar_pairs,list(set(a)^set(add_matched)^set(same_add)),list(set(b)^set(removed_matched)^set(same_removed))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_modification(modified):\n",
    "    if modified == []:\n",
    "        return {}\n",
    "    # flattern list of dicts\n",
    "    modified = dict(ChainMap(*modified))\n",
    "    return modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_refs(d):\n",
    "    added_res = d['refs.added']\n",
    "    removed_res = d['refs.removed']\n",
    "    modified = d['refs.modified']\n",
    "    # change the format \n",
    "    modified = update_modification(modified)\n",
    "    # if one of added and removed is none, then do not need to compare and update the modification\n",
    "    # just needto update modified\n",
    "    if (added_res == []) | (removed_res == []):\n",
    "        return [modified,added_res,removed_res]\n",
    "    \n",
    "    x = find_sim_pairs(added_res,removed_res)\n",
    "    m = {}\n",
    "    a = []\n",
    "    r = []\n",
    "    \n",
    "    if x is not None:\n",
    "        m = x[0]\n",
    "        a = x[1]\n",
    "        r = x[2]\n",
    "    m.update(modified)\n",
    "    return [m,a,r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_ref_df = df.drop(columns={'is_talk','revision.parentid'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_ref_df[['modified','added','removed']] = page_ref_df.apply(lambda d: pd.Series(process_refs(d)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_ref_amr = page_ref_df[['added','modified','removed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_ref_amr_df = ddf.from_pandas(fin_ref_amr,chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_ref_amr_df.to_parquet('../../intermediate-result/TCM//refs-added-modified-removed-final',object_encoding='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the references\n",
    "for each added and modifed ref, process the ref, and get the editor-article-ref info <br>\n",
    "match the references with same ref id (process each reference to get a ref_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_created_id = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generated_id(title): \n",
    "    tmp_id = randint(100000, 999999)\n",
    "    # if self generated id is already used, generate another one\n",
    "    while (title + '.' + 'id.' + str(tmp_id) in self_created_id):\n",
    "        tmp_id = randint(100000, 999999)\n",
    "    ref_id = title + '.' + 'id.' + str(tmp_id)\n",
    "    self_created_id.append(ref_id)\n",
    "    return ref_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_content(ref):\n",
    "    wikicode = mwparserfromhell.parse(ref)\n",
    "    tag = wikicode.filter_tags()\n",
    "    if len(tag) == 0:\n",
    "        return re.search(r'>(.*?)<', ref).group(1)\n",
    "    return tag[0].contents.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ref_with_name(ref):\n",
    "    tag_name = find_tag_name(ref)\n",
    "    tag_content = get_tag_content(ref)\n",
    "    return [tag_name,tag_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref_list: reference list, key: ref id, value: parsed ref content \n",
    "# res: result list, which ref is added by which user to which article\n",
    "# matched_ids: matched id pairs, key: old id, value: new id \n",
    "# ref_id_pairs: key: unparsed ref content, value: ref id, use unparsed since parsed content is complex of being a key\n",
    "ref_list = {}\n",
    "res = []\n",
    "matched_ids = {}\n",
    "ref_id_pairs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add ref info to ref list\n",
    "def insert_to_dicts(rid,content,title,user):\n",
    "    # if content is already in ref list (use ref_id_pairs to check it)\n",
    "    # use new id and replace old id\n",
    "    if content in ref_id_pairs:\n",
    "        old_id = ref_id_pairs[content]\n",
    "\n",
    "        # second condition is used to prevent loops\n",
    "        # if old is is already used as a key in matched_ids (matched other ids)\n",
    "        # then do nothing\n",
    "        if (old_id != rid) & (rid not in matched_ids):    \n",
    "            matched_ids[old_id] = rid\n",
    "            ref_id_pairs[content] = rid\n",
    "            ref_list[rid] = parse_content(content)\n",
    "\n",
    "            ref_list.pop(old_id, None)\n",
    "            res.append({'ref_id':rid,'page.title':title,'contributor':user})\n",
    "            all_vals = list(matched_ids.values())\n",
    "            if old_id in all_vals:\n",
    "                # get key and replace value\n",
    "                # get list of keys, may have multiple keys\n",
    "                keys = [k for k, v in matched_ids.items() if v == old_id]\n",
    "                for key in keys:\n",
    "                    if key != rid:\n",
    "                        matched_ids[key] = rid\n",
    "\n",
    "    # else a new ref, then use rid and add refs to both lists\n",
    "    else:\n",
    "        ref_list[rid] = parse_content(content)\n",
    "        ref_id_pairs[content] = rid\n",
    "        res.append({'ref_id':rid,'page.title':title,'contributor':user})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the ref and add ref to ref list\n",
    "# modified_id are used to process modified refs\n",
    "def process_single_ref(ref,title,user,modified_id):\n",
    "    [tag_name, tag_content] = extract_ref_with_name(ref)\n",
    "\n",
    "    if tag_name is None:\n",
    "        parsed = parse_content(tag_content)\n",
    "        ref_id = ''\n",
    "        if modified_id!='':\n",
    "            ref_id = modified_id\n",
    "        elif ('ref_id' in parsed) and (parsed['ref_id'] is not None):\n",
    "            ref_id = parsed['ref_id']\n",
    "        else:\n",
    "            ref_id = generated_id(title)\n",
    "        insert_to_dicts(ref_id,tag_content,title,user)\n",
    "    else:\n",
    "        # has a tag name\n",
    "        ref_id = title + '.' + tag_name\n",
    "        # if no content, add to result list\n",
    "        if tag_content == '':\n",
    "            res.append({'ref_id':ref_id ,'page.title':title,'contributor':user})\n",
    "        else:\n",
    "            parsed = parse_content(tag_content)\n",
    "            # modified ref\n",
    "            if modified_id != '':\n",
    "                insert_to_dicts(modified_id,tag_content,title,user)\n",
    "            # use title + tagName as ref id\n",
    "            else:\n",
    "                insert_to_dicts(ref_id,tag_content,title,user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_added_refs(d):\n",
    "    added_refs = d['added']\n",
    "    for ref in added_refs:\n",
    "        process_single_ref(ref,d['page.title'],d['contributor.username'],'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = page_ref_df.apply(lambda d: process_added_refs(d),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_modified_refs(d):\n",
    "    modified = d['modified']\n",
    "    title = d['page.title']\n",
    "    user = d['contributor.username']\n",
    "    # the added refs should already in ref_list\n",
    "    # just need to find proper ids and update the new info in ref list, ref id pairs, and page user ref info \n",
    "    if modified == {}:\n",
    "        return\n",
    "    for old, new in modified.items():\n",
    "        # if old id in ref_id_pairs, use old id for the new ref\n",
    "        if old in ref_id_pairs:\n",
    "            ref_id = ref_id_pairs[old]\n",
    "            process_single_ref(new,title,user,ref_id)\n",
    "        # else not in ref_id_pairs, add as a new\n",
    "        else:\n",
    "            process_single_ref(new,title,user,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = page_ref_df.apply(lambda d: process_modified_refs(d),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare refs and match similar refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. compare urls\n",
    "x = pd.DataFrame.from_dict(ref_list,orient='index')\n",
    "x = x.fillna('')\n",
    "x['ref_id'] = x.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url_prefix(url):\n",
    "    t = re.compile(r\"(https?://)?(www.)?\")\n",
    "    return t.sub('', url).strip().strip('/').lower()\n",
    "x['url'] = x['url'].apply(remove_url_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_index(index):\n",
    "    return list(set(index))\n",
    "url = x.groupby('url').agg({'ref_id':get_unique_index})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the column with no url\n",
    "url = url[url.index != '']\n",
    "# all pairs with same url\n",
    "same_urls_pairs = url['ref_id'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the format of similar pairs\n",
    "# for all similar pairs, use one as the final version\n",
    "def sim_pairs(pairs):\n",
    "    sim_pairs = {}\n",
    "    for item in pairs:\n",
    "        if len(item) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            val_index = 0\n",
    "            for i in range(0,len(item)):\n",
    "                # keep the reference with better id\n",
    "                if item[i].startswith('pmid'):\n",
    "                    val_index = i\n",
    "                    break\n",
    "                if item[i].startswith('doi'):\n",
    "                    val_index = i\n",
    "                    break\n",
    "                if item[i].startswith('isbn'):\n",
    "                    val_index = i\n",
    "                    break\n",
    "                # tag name that does not contain .id.\n",
    "                if re.search('.id.', item[i]) == None:\n",
    "                    val_index = i\n",
    "                    break\n",
    "            for i in range(0,len(item)):\n",
    "                if i == val_index:\n",
    "                    continue\n",
    "                else:\n",
    "                    sim_pairs[item[i]] = item[val_index]\n",
    "                \n",
    "    return sim_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dict = sim_pairs(same_urls_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_res(drop_index,replace_list):\n",
    "    for item in res:\n",
    "        if item['ref_id'] in drop_index:\n",
    "            item['ref_id'] = replace_list[item['ref_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update based on matched_ids from previous part\n",
    "drop_index = list(matched_ids.keys())\n",
    "update_res(drop_index,matched_ids)\n",
    "\n",
    "# update url_dict to prevent loops \n",
    "url_dict = {k:v for k,v in url_dict.items() if v not in matched_ids}\n",
    "drop_index = list(url_dict.keys())\n",
    "update_res(drop_index,url_dict)\n",
    "\n",
    "# add url_dict to matched_ids\n",
    "matched_ids.update(url_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. For formatted refs, compare title, author, publisher, etc. \n",
    "formatted = x[x['unparsed'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each type, we need to look at the title, if tht titles are similar, then compare other criteria\n",
    "def similar_fields(df,field,tol):\n",
    "    possible_pairs = []\n",
    "    field_list = df[field].to_list()\n",
    "    df_index = df.index.to_list()\n",
    "    exist_index = []\n",
    "    for i in range(len(field_list)):\n",
    "        curr_index = df_index[i]\n",
    "        curr_val = field_list[i]\n",
    "        if curr_index in exist_index:\n",
    "            continue\n",
    "        if len(curr_val) == 0:\n",
    "            continue\n",
    "        ratio = df.apply(lambda d: get_distance(d[field],curr_val),axis=1)\n",
    "#         ratio = x/len(curr_title)\n",
    "        tmp = []\n",
    "        for i in range(len(ratio)):\n",
    "            curr_r = ratio[i]\n",
    "            if curr_r < tol:\n",
    "                tmp.append(df_index[i])\n",
    "                exist_index.append(df_index[i])\n",
    "        if len(tmp) > 1:\n",
    "            possible_pairs.append(tmp)\n",
    "    return possible_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_cols(df,possible_pairs,metric,tol):\n",
    "    sim_pairs = []\n",
    "    \n",
    "    for p in possible_pairs:\n",
    "        tem_pairs = []\n",
    "        x = list(combinations(p,2))\n",
    "\n",
    "        for item in x:\n",
    "            d1 = df.loc[item[0]][metric]\n",
    "#             d1 = re.sub('[^A-Za-z0-9]+', '', d1)\n",
    "            \n",
    "\n",
    "            d2 = df.loc[item[1]][metric]\n",
    "#             d2 = re.sub('[^A-Za-z0-9]+', '', d2)\n",
    "            \n",
    "\n",
    "            change_rate = get_distance(d1,d2)\n",
    "\n",
    "            if change_rate <= tol:\n",
    "                tem_pairs.append(item[0])\n",
    "                tem_pairs.append(item[1])\n",
    "            \n",
    "        pairs = list(set(tem_pairs))\n",
    "        sim_pairs.append(pairs)\n",
    "            \n",
    "    return [x for x in sim_pairs if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_pairs = similar_fields(formatted,'title',0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare some import parts\n",
    "filter_publisher = compare_cols(formatted,possible_pairs,'publisher',0.3) \n",
    "filter_author = compare_cols(formatted,possible_pairs,'author',0.3)\n",
    "filter_journal = compare_cols(formatted,possible_pairs,'journal',0.3)\n",
    "filter_work = compare_cols(formatted,possible_pairs,'work',0.3)\n",
    "filter_url= compare_cols(formatted,possible_pairs,'url',0.1)\n",
    "possible_pairs = list(set().union((tuple(row) for row in filter_publisher),(tuple(row) for row in filter_author),(tuple(row) for row in filter_journal),(tuple(row) for row in filter_work),(tuple(row) for row in filter_url)))\n",
    "possible_pairs = [list(t) for t in possible_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dict = sim_pairs(possible_pairs)\n",
    "replace_dict = {k:v for k,v in replace_dict.items() if v not in matched_ids}\n",
    "drop_index = list(replace_dict.keys())\n",
    "update_res(drop_index,replace_dict)\n",
    "\n",
    "matched_ids.update(replace_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. compare content for all unformatted try to find similar one. \n",
    "unformatted = x[x['unparsed'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_pairs = similar_fields(unformatted,'unparsed',0.5)\n",
    "replace_dict = sim_pairs(possible_pairs)\n",
    "replace_dict = {k:v for k,v in replace_dict.items() if v not in matched_ids}\n",
    "drop_index = list(replace_dict.keys())\n",
    "update_res(drop_index,replace_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_ids.update(replace_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. For unformatted refs, find if there is a match with formatted refs (search in unforamtted refs with existing \n",
    "#    ids/authors in formatted)\n",
    "def find_match_with_format(content,exist_field):\n",
    "    for i in exist_field.index:\n",
    "        x = exist_field[i]\n",
    "        if x.lower() in content.lower():\n",
    "            return i\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_sim_pairs(id1,id2,l):\n",
    "    l.append([id1,id2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-103-8b75007bb327>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unformatted['matched_pmid'] = unformatted.apply(lambda d: find_match_with_format(d['unparsed'],exist_pmid),axis=1)\n"
     ]
    }
   ],
   "source": [
    "if 'pmid' in formatted:\n",
    "    exist_pmid = formatted[formatted['pmid']!='']['pmid']\n",
    "    unformatted['matched_pmid'] = unformatted.apply(lambda d: find_match_with_format(d['unparsed'],exist_pmid),axis=1)\n",
    "    matched_pmid_df = unformatted[unformatted['matched_pmid']!='']\n",
    "    matched_pmid = []\n",
    "    c = matched_pmid_df.apply(lambda d: form_sim_pairs(d['ref_id'],d['matched_pmid'],matched_pmid),axis=1)\n",
    "    replace_dict = sim_pairs(matched_pmid)\n",
    "    replace_dict = {k:v for k,v in replace_dict.items() if v not in matched_ids}\n",
    "    matched_ids.update(replace_dict)\n",
    "    drop_index = list(replace_dict.keys())\n",
    "    update_res(drop_index,replace_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-104-fa0a45554044>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unformatted['matched_authors'] = unformatted.apply(lambda d: find_match_with_format(d['unparsed'],exist_author),axis=1)\n"
     ]
    }
   ],
   "source": [
    "# author\n",
    "exist_author = formatted[formatted['author']!='']['author']\n",
    "unformatted['matched_authors'] = unformatted.apply(lambda d: find_match_with_format(d['unparsed'],exist_author),axis=1)\n",
    "matched_authors_df = unformatted[unformatted['matched_authors']!='']\n",
    "\n",
    "matched_authors= []\n",
    "d = matched_authors_df.apply(lambda d: form_sim_pairs(d['ref_id'],d['matched_authors'],matched_authors),axis=1)\n",
    "\n",
    "replace_dict = sim_pairs(matched_authors)\n",
    "replace_dict = {k:v for k,v in replace_dict.items() if v not in matched_ids}\n",
    "matched_ids.update(replace_dict)\n",
    "drop_index = list(replace_dict.keys())\n",
    "update_res(drop_index,replace_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-105-9fa1aede0028>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unformatted['matched_isbn'] = unformatted.apply(lambda d: find_match_with_format(d['unparsed'],exist_isbn),axis=1)\n"
     ]
    }
   ],
   "source": [
    "# isbn\n",
    "if 'isbn' in formatted:\n",
    "    exist_isbn = formatted[formatted['isbn']!='']['isbn']\n",
    "    unformatted['matched_isbn'] = unformatted.apply(lambda d: find_match_with_format(d['unparsed'],exist_isbn),axis=1)\n",
    "\n",
    "    matched_isbn_df = unformatted[unformatted['matched_isbn']!='']\n",
    "    matched_isbn= []\n",
    "    e = matched_isbn_df.apply(lambda d: form_sim_pairs(d['ref_id'],d['matched_isbn'],matched_isbn),axis=1)\n",
    "\n",
    "\n",
    "    replace_dict = sim_pairs(matched_isbn)\n",
    "    replace_dict = {k:v for k,v in replace_dict.items() if v not in matched_ids}\n",
    "    matched_ids.update(replace_dict)\n",
    "    drop_index = list(replace_dict.keys())\n",
    "    update_res(drop_index,replace_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-106-dd82ea55004c>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unformatted['matched_doi'] = unformatted.apply(lambda d: find_match_with_format(d['unparsed'],exist_doi),axis=1)\n"
     ]
    }
   ],
   "source": [
    "# doi\n",
    "if 'doi' in formatted:\n",
    "    exist_doi = formatted[formatted['doi']!='']['doi']\n",
    "    unformatted['matched_doi'] = unformatted.apply(lambda d: find_match_with_format(d['unparsed'],exist_doi),axis=1)\n",
    "\n",
    "    matched_doi_df = unformatted[unformatted['matched_doi']!='']\n",
    "    matched_doi= []\n",
    "    f = matched_doi_df.apply(lambda d: form_sim_pairs(d['ref_id'],d['matched_doi'],matched_doi),axis=1)\n",
    "    replace_dict = sim_pairs(matched_doi)\n",
    "    replace_dict = {k:v for k,v in replace_dict.items() if v not in matched_ids}\n",
    "    matched_ids.update(replace_dict)\n",
    "    drop_index = list(replace_dict.keys())\n",
    "    update_res(drop_index,replace_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ref_list = pd.DataFrame.from_dict(ref_list,orient='index')\n",
    "res = pd.DataFrame(res)\n",
    "final_ref_list = final_ref_list[final_ref_list.index.isin(res['ref_id'].unique())]\n",
    "res_df = ddf.from_pandas(res,chunksize=10000)\n",
    "final_ref_list_df = ddf.from_pandas(final_ref_list,chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_parquet('../../intermediate-result/TCM/TCM-ref-page-user-info')\n",
    "final_ref_list_df.to_parquet('../../intermediate-result/TCM/TCM-ref-list-info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get former (added, modified, removed) for TCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref_former(added, modified, removed):\n",
    "    return [len(added), len(modified),len(removed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_ref_df[['ref_added','ref_modified','ref_removed']] =\\\n",
    "page_ref_df.apply(lambda d: pd.Series(get_ref_former(d['added'],d['modified'],d['removed'])),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = page_ref_df[['ref_added','ref_modified','ref_removed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = ddf.from_pandas(x,chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df.to_parquet('../../intermediate-result/TCM/TCM-ref-former')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_info_top_users = res[res['contributor'].isin(selected_editors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_contributions(d):\n",
    "    res = {}\n",
    "    for item in d:\n",
    "        if item not in res:\n",
    "            res[item] = 1\n",
    "        else:\n",
    "            res[item] += 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dict = ref_info_top_users.groupby(['ref_id']).apply(lambda d: count_contributions(d['contributor']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dict = pd.DataFrame(ref_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_info_top_users_groupped = ref_info_top_users.groupby(['ref_id']).count().drop(columns={'contributor','page.title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_info_top_users_groupped = ref_info_top_users_groupped.join(ref_dict).rename(columns={0:'ref_dict'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(selected_editors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "bibcoupling = np.zeros((n,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dict = dict(enumerate(x.rstrip() for x in selected_editors))\n",
    "user_dict = dict((y,x) for x,y in user_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = ref_info_top_users_groupped.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure index 1 is smaller than index 2\n",
    "# only calculate half of the matrix, the other is symmetric \n",
    "from itertools import combinations\n",
    "for ref in refs:\n",
    "    curr_dict = ref_info_top_users_groupped.loc[ref]['ref_dict']\n",
    "    if len(curr_dict) < 2:\n",
    "        continue\n",
    "    all_user_pairs = list(combinations([*curr_dict],2))\n",
    "    for pair in all_user_pairs:\n",
    "        u1 = pair[0]\n",
    "        u2 = pair[1]\n",
    "        val = min(curr_dict[u1],curr_dict[u2])\n",
    "        \n",
    "        index_1 = user_dict[u1]\n",
    "        index_2 = user_dict[u2]\n",
    "        \n",
    "        bibcoupling[index_1][index_2] += val\n",
    "        bibcoupling[index_2][index_1] += val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.DataFrame(bibcoupling,index=selected_editors,columns=selected_editors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.drop(matrix.loc[matrix.sum(axis=1)==0].index, inplace=True)\n",
    "matrix.drop(columns=matrix.columns[matrix.sum()==0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fill_diagonal(matrix.values,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.to_parquet('../../result/TCM/TCM-bibliography.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.to_csv(\"../../result/TCM/TCM-bibliography.tsv\", sep=\"\\t\",encoding='utf-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
