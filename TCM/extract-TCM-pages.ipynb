{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import dask\n",
    "import dask.dataframe as ddf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mwparserfromhell\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get articles in selected categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikipediaCategoryArticleNames(category):\n",
    "    categoryPage= requests.get('https://en.wikipedia.org/wiki/Category:'+category)\n",
    "    cpxml= BeautifulSoup(categoryPage.text,'html')\n",
    "    span = cpxml.select(\"span#Pages_in_category\")[0]\n",
    "    page_list = span.find_next(\"div\")\n",
    "    # get titles instead of href, since we want to get content from processed wiki dump\n",
    "    test=[ref.replace('/wiki/','') \n",
    "          for ref in \n",
    "          [a['title'] \n",
    "           for a in page_list.find_all('a') \n",
    "           if a.has_attr('href') and a.has_attr('title') and len(a.attrs)==2]\n",
    "          if not ':' in ref and not '?' in ref and not '//' in ref]\n",
    "    return set(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcmCategories=set(['Traditional_Chinese_medicine', \n",
    "                   'Acupuncture',\n",
    "                   'Baguazhang',\n",
    "                   'Chinese_medical_texts',\n",
    "                   'Plants_used_in_traditional_Chinese_medicine', \n",
    "                   'Fungi_used_in_traditional_Chinese_medicine',\n",
    "                   'Qigong',\n",
    "                   'Tai_chi',\n",
    "#                    'Traditional_chinese_medical_pills',\n",
    "                   'Wu_Xing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcmArticles= {category: wikipediaCategoryArticleNames(category) for category in tcmCategories}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [*tcmArticles.values()]\n",
    "titles = [list(i) for i in titles]\n",
    "titles = sum(titles, [])\n",
    "titles = list(set(titles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get main article\n",
    "Look up the most curent content for each article, then search for {{Main|xx}} markup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../intermediate-result/title-look-up.pickle', 'rb') as handle:\n",
    "    title_lookup = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_info = {} # which folder contains which article\n",
    "articles_not_in_wiki_dump = [] # articles cannot find in wiki dumps, need to extract using API?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_folder_info(folder,title):\n",
    "    if folder in folder_info:\n",
    "        folder_info[folder].append(title)\n",
    "    else:\n",
    "        folder_info[folder] = [title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_articles(title):\n",
    "    if title not in title_lookup:\n",
    "        articles_not_in_wiki_dump.append(title)\n",
    "        return []\n",
    "    added_pages = []\n",
    "    folder = title_lookup[title]\n",
    "    article_content = ddf.read_parquet('../../../mount-files/all-data-extracted-page-info/' + folder +'/page.content').compute().loc[title]\n",
    "    wikicode = mwparserfromhell.parse(article_content)\n",
    "    templates = wikicode.filter_templates()\n",
    "    for i in templates:\n",
    "        if i.name.lower() == 'main':\n",
    "            for j in i.params:\n",
    "                if (j not in titles) and (j not in added_pages):\n",
    "                    added_pages.append(j.strip())\n",
    "    return added_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_pages = []\n",
    "added_info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in titles:\n",
    "    added = get_main_articles(title)\n",
    "    if len(added) > 0:\n",
    "        added_pages += added\n",
    "        added_info[title] = added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_pages = list(set(added_pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles += added_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../intermediate-result/TCM/tcmTitles.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(titles, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Article Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../../enwiki-columns/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_folders = glob(path + '*.bz2*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_files = glob(all_folders[0] + '/' + '*bz20--2499*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [i.split('parquet_dir_')[1] for i in sub_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields.remove('page.title')\n",
    "fields.remove('revision.fileindex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_path = '../../tcm-columns-add-main/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in fields:\n",
    "    field_path = final_path + i\n",
    "    if not os.path.exists(field_path):\n",
    "        os.makedirs(field_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each article, get the folder name\n",
    "# then extract the info for all fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce file reading time: some titles are from one folder, only need to read once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_title_info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_folder_title_info(title,folder):\n",
    "    if folder in folder_title_info:\n",
    "        folder_title_info[folder].append(title)\n",
    "    else:\n",
    "        folder_title_info[folder] = [title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in titles:\n",
    "    if title in title_lookup:\n",
    "        folder = title_lookup[title]\n",
    "        update_folder_title_info(title,folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bznumber(text):\n",
    "    match = re.search(r'(\\bbz(\\d+--\\d+)\\b)',text).group(0)\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extractTCM(object):\n",
    "    def __init__(self,folder,kept_titles):\n",
    "        self.folder = folder\n",
    "        self.kept_titles = kept_titles\n",
    "        \n",
    "    def get_page_info(self):\n",
    "        all_titles_files = glob(path + self.folder + '/' + '*page.title*')\n",
    "        page_title_files = [s for s in all_titles_files]\n",
    "        titles = [ddf.read_parquet(file) for file in page_title_files]\n",
    "        self.page_title = ddf.concat(titles).compute()\n",
    "        \n",
    "        self.unique_files = list(map(find_bznumber, all_titles_files))\n",
    "        \n",
    "        file_index_files = [s for s in glob(path + self.folder + '/' + '*revision.fileindex*')]\n",
    "        file_index = [ddf.read_parquet(file) for file in file_index_files]\n",
    "        self.file_index = ddf.concat(file_index).compute()\n",
    "        \n",
    "        page_info = self.page_title.join(self.file_index)\n",
    "        self.page_info = page_info[page_info['page.title'].isin(self.kept_titles)]\n",
    "        \n",
    "    def find_file_number(self,min_ind,max_ind):\n",
    "            min_val = int(min_ind/2500) * 2500\n",
    "            max_val = int(max_ind/2500) * 2500\n",
    "            res = []\n",
    "            while min_val != max_val:\n",
    "                res.append([i for i in self.unique_files if i.startswith('bz2' + str(min_val) + '--')][0])\n",
    "                min_val += 2500\n",
    "            res.append([i for i in self.unique_files if i.startswith('bz2' + str(min_val) + '--')][0])\n",
    "            return res\n",
    "        \n",
    "    def get_kept_file_index(self):\n",
    "        self.kept_page_info = self.page_info.groupby('page.title').agg({'revision.fileindex':['min','max']})\n",
    "        self.kept_page_info.columns = self.kept_page_info.columns.droplevel(0)        \n",
    "        max_ind = self.file_index.max()['revision.fileindex']\n",
    "        max_file = [i for i in self.unique_files if i.endswith(str(max_ind))][0]\n",
    "        self.kept_page_info['file_index'] = self.kept_page_info.apply(lambda d: self.find_file_number(d['min'],d['max']),axis=1)\n",
    "    \n",
    "    def extract_other_fields(self,kept_index,field,fileindex,title):\n",
    "        fileindex = ['*' + i +'*' for i in fileindex]\n",
    "        all_folders = []\n",
    "        for i in fileindex:\n",
    "            all_folders += glob(path + self.folder + '/' + i)\n",
    "        kept_folders = [i for i in all_folders if field in i]\n",
    "        all_files = [s for s in kept_folders if field in s]\n",
    "        df = [ddf.read_parquet(file) for file in all_files]\n",
    "        df = ddf.concat(df).compute()\n",
    "        df = df[df.index.isin(kept_index)]\n",
    "        dask_df = ddf.from_pandas(df,chunksize=1000)\n",
    "        fin_path = final_path + field +'/' + title + '/'\n",
    "        dask_df.to_parquet(fin_path)\n",
    "\n",
    "    def get_title_info(self,title):\n",
    "        # get and save title info\n",
    "        curr_page_info = self.page_info[self.page_info['page.title'] == title]\n",
    "        curr_page_title = curr_page_info[['page.title']]\n",
    "        curr_page_title_df = ddf.from_pandas(curr_page_title,chunksize=1000)\n",
    "        curr_fin_path_title = final_path + 'page.title/' + title + '/'\n",
    "        curr_page_title_df.to_parquet(curr_fin_path_title)\n",
    "        # get and save fileindex info\n",
    "        curr_fileindex = curr_page_info[['revision.fileindex']]\n",
    "        curr_fileindex_df = ddf.from_pandas(curr_fileindex,chunksize=1000)\n",
    "        curr_fin_path_fileindex = final_path + 'revision.fileindex/' + title + '/'\n",
    "        curr_fileindex_df.to_parquet(curr_fin_path_fileindex)\n",
    "        curr_kept_index = list(curr_page_info.index)\n",
    "        curr_fileindex_list = self.kept_page_info.loc[title]['file_index']\n",
    "        x = [self.extract_other_fields(curr_kept_index,i,curr_fileindex_list,title) for i in fields]\n",
    "        \n",
    "    def run(self):\n",
    "        self.get_page_info()\n",
    "        self.get_kept_file_index()\n",
    "        x = [self.get_title_info(t) for t in self.kept_titles]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in folder_title_info.items():\n",
    "    x = extractTCM(k,v)\n",
    "    x.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
